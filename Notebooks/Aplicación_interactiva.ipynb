{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Celda 1: Instalaci√≥n de librer√≠as\n",
        "\n"
      ],
      "metadata": {
        "id": "1HaeiFsD7h5A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tu6WKGVG7UXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9d425a-7df0-407e-ac43-247ca598f582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# 1. Instalamos las librer√≠as necesarias\n",
        "# Gradio: Para la interfaz visual\n",
        "# Deep-translator: Para traducir espa√±ol -> ingl√©s (ya que entrenamos con datos en ingl√©s)\n",
        "!pip install -q gradio deep-translator transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 2: Carga del Modelo desde Drive\n",
        "La celda conecta con Google Drive y carga el modelo que entrenamos en el Notebook 2."
      ],
      "metadata": {
        "id": "_ZiQLZUh7l5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from google.colab import drive\n",
        "\n",
        "# 2. Montar Google Drive\n",
        "# Si ya est√° montado, esto no har√° nada, si no, pedir√° permisos.\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# --- CONFIGURACI√ìN ---\n",
        "# Aseg√∫rate de que esta ruta sea EXACTAMENTE donde guardaste el modelo en el Notebook 2\n",
        "MODEL_DIR = \"/content/drive/MyDrive/proyecto_ia/modelos/sentimiento_xlmroberta_v1\"\n",
        "\n",
        "print(f\"üîÑ Cargando modelo desde: {MODEL_DIR} ...\")\n",
        "\n",
        "try:\n",
        "    # Cargar Tokenizer y Modelo\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
        "\n",
        "    # Configurar dispositivo (GPU si est√° disponible, si no CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    print(f\"‚úÖ Modelo cargado exitosamente. Corriendo en: {device}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR CR√çTICO: No se pudo cargar el modelo.\")\n",
        "    print(f\"Detalle: {e}\")\n",
        "    print(\"\\nüí° SOLUCI√ìN: Verifica que la carpeta '{MODEL_DIR}' exista en tu Drive y contenga los archivos config.json, model.safetensors, etc. Aseg√∫rate de que los permisos de acceso a Google Drive sean correctos y que la ruta sea accesible.\") # Added more specific advice here\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwfLQNxE7Vls",
        "outputId": "78363a8e-85fe-4830-9808-9d2f844811be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "üîÑ Cargando modelo desde: /content/drive/MyDrive/proyecto_ia/modelos/sentimiento_xlmroberta_v1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/proyecto_ia/modelos/sentimiento_xlmroberta_v1' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Modelo cargado exitosamente. Corriendo en: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celda 3: L√≥gica y App (Interfaz Gr√°fica)\n",
        "Celda final. Al ejecutarla, aparecer√° la aplicaci√≥n interactiva."
      ],
      "metadata": {
        "id": "9YS_fqlR7uDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch.nn.functional as F\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# --- CONFIGURACI√ìN DE LA APP ---\n",
        "\n",
        "# Inicializar traductor (Espa√±ol -> Ingl√©s)\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "\n",
        "# Mapeo de etiquetas (0, 1, 2) a texto legible\n",
        "labels_map = {\n",
        "    0: \"Negativo üò†\",\n",
        "    1: \"Neutro üòê\",\n",
        "    2: \"Positivo üòÉ\"\n",
        "}\n",
        "\n",
        "def predecir_sentimiento(comentario):\n",
        "    \"\"\"\n",
        "    Funci√≥n que recibe el texto del usuario, lo traduce,\n",
        "    lo pasa por el modelo y devuelve las probabilidades.\n",
        "    \"\"\"\n",
        "    if not comentario:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # 1. Traducir al ingl√©s (necesario por el entrenamiento previo)\n",
        "        texto_en = translator.translate(comentario)\n",
        "    except Exception as e:\n",
        "        return {\"Error de conexi√≥n/traducci√≥n\": 0.0}\n",
        "\n",
        "    # 2. Tokenizar\n",
        "    inputs = tokenizer(\n",
        "        texto_en,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512\n",
        "    ).to(device)\n",
        "\n",
        "    # 3. Inferencia (Predicci√≥n)\n",
        "    model.eval() # Modo evaluaci√≥n\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # 4. Convertir logits a probabilidades (0 a 1)\n",
        "    probs = F.softmax(logits, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    # 5. Formatear para Gradio (Diccionario {Etiqueta: Probabilidad})\n",
        "    # Esto le dice a Gradio qu√© barras dibujar\n",
        "    resultado_dict = {\n",
        "        labels_map[0]: float(probs[0]),\n",
        "        labels_map[1]: float(probs[1]),\n",
        "        labels_map[2]: float(probs[2])\n",
        "    }\n",
        "\n",
        "    return resultado_dict\n",
        "\n",
        "# --- CREACI√ìN DE LA INTERFAZ ---\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=predecir_sentimiento, # La funci√≥n cerebro\n",
        "    inputs=gr.Textbox(\n",
        "        lines=3,\n",
        "        placeholder=\"Escribe aqu√≠ tu rese√±a (ej: Me encant√≥ el producto...)\",\n",
        "        label=\"Tu Rese√±a\"\n",
        "    ),\n",
        "    outputs=gr.Label(num_top_classes=3, label=\"Probabilidades\"), # Gr√°fico de barras\n",
        "    title=\"üõí Analizador de Sentimientos (Amazon Demo)\",\n",
        "    description=\"\"\"\n",
        "    **Instrucciones:** Escribe una opini√≥n sobre un producto en Espa√±ol.\n",
        "    El sistema traducir√° internamente y analizar√° si es Positiva, Negativa o Neutra.\n",
        "    \"\"\",\n",
        "    examples=[\n",
        "        [\"El producto lleg√≥ roto y la caja estaba abierta. P√©simo servicio.\"],\n",
        "        [\"Es incre√≠ble, la calidad es superior a lo que esperaba. Recomendado.\"],\n",
        "        [\"Cumple su funci√≥n, pero tard√≥ un poco en llegar. Es un producto normal.\"],\n",
        "        [\"No estoy seguro si me gusta, tiene cosas buenas y malas.\"]\n",
        "    ],\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "# Lanzar la app\n",
        "# debug=True ayuda a ver errores si algo falla\n",
        "# share=True genera un link p√∫blico temporal (√∫til para mostrar a otros)\n",
        "interface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "BksYXYvm7eJE",
        "outputId": "34f4412c-c3ae-4ecd-c8e6-9c2a11554ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a0dbcc3959416dc849.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a0dbcc3959416dc849.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
# ğŸ›’ AnÃ¡lisis de Sentimientos en ReseÃ±as de Amazon con XLM-Roberta

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![HuggingFace](https://img.shields.io/badge/Hugging%20Face-Transformers-orange)
![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red)
![Status](https://img.shields.io/badge/Status-Prototipo-yellow)

Este proyecto implementa un modelo de **Procesamiento de Lenguaje Natural (NLP)** capaz de clasificar reseÃ±as de productos de Amazon en tres categorÃ­as: **Positivo, Neutro y Negativo**. 

Utiliza el modelo pre-entrenado **XLM-Roberta** y una estrategia de traducciÃ³n para permitir inferencia multilingÃ¼e (EspaÃ±ol/InglÃ©s).

## ğŸ—‚ï¸ Estructura del Proyecto

El flujo de trabajo se divide en 4 etapas principales (Notebooks):

**Lenguaje y Entorno:**
* Python 3.10+: Lenguaje base para todo el procesamiento.
* Google Colab: Entorno de ejecuciÃ³n en la nube con aceleraciÃ³n por hardware (GPU T4) para el entrenamiento del Transformer.

El laboratorio consta de 3 notebooks principales ubicados en la carpeta `Notebooks`:

## ğŸ“˜ [01_Eda_y_limpieza](Notebooks/01_Eda_y_limpieza.ipynb)

* **Ingesta del dataset Amazon Product Reviews.**

* **Limpieza de texto con Expresiones Regulares (Regex).**

* **EstratificaciÃ³n de datos (Train/Test Split).**

## ğŸ“™ [02 Entranamiento](Notebooks/02_Entrenamiento_modelo.ipynb)

* **ConfiguraciÃ³n del Tokenizador AutoTokenizer.**

* **Entrenamiento con la API Trainer de Hugging Face (GPU T4).**

* **Persistencia del modelo entrenado.**

## ğŸ“— [03 EvaluaciÃ³n](03_Evaluacion_comparacion.ipynb)

* **EvaluaciÃ³n de mÃ©tricas (Matriz de ConfusiÃ³n, F1-Score).**

* **FunciÃ³n de predicciÃ³n final para consumo del modelo con traducciÃ³n integrada.**

## ğŸ“Š Dataset

Se utilizÃ³ el conjunto de datos **Amazon Product Reviews** disponible en Kaggle.
* **Total de muestras:** ~21,000 reseÃ±as.
* **Clases:** * `Positive` (2): ~18,800
    * `Negative` (0): ~2,100
    * `Neutral` (1): ~300

> âš ï¸ **Nota:** El dataset presenta un fuerte desbalance de clases, predominando masivamente las reseÃ±as positivas.

## ğŸ› ï¸ TecnologÃ­as Utilizadas

* **Python** (Entorno Google Colab)
* **Transformers (Hugging Face):** Para el modelo XLM-Roberta y Tokenizer.
* **PyTorch:** Backend de Deep Learning.
* **Scikit-Learn:** Para mÃ©tricas y divisiÃ³n de datos.
* **Deep-Translator:** Para pipeline de traducciÃ³n en inferencia (ES -> EN).
* **Pandas & Matplotlib:** ManipulaciÃ³n y visualizaciÃ³n de datos.

## ğŸš€ InstalaciÃ³n y Uso

1. **Clonar el repositorio:**
   ```bash
   git clone [https://github.com/tu-usuario/amazon-sentiment-analysis.git](https://github.com/tu-usuario/amazon-sentiment-analysis.git)
   cd amazon-sentiment-analysis

## ğŸ‘¥ Autores - Grupo 3

* Liz Eliana Castillo Zamora

* Pablo Mauricio Castro Hinostroza

* Erick SebastiÃ¡n Rivas

* Ãngel Israel Romero Medina

**Proyecto acadÃ©mico para la asignatura de Inteligencia Artificial - UEES.**

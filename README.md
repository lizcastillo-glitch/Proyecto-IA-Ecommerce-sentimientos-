# An√°lisis de Sentimientos H√≠brido para E-Commerce üåéüõí

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Hugging Face](https://img.shields.io/badge/ü§ó%20Transformers-XLM--RoBERTa-yellow)
![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red)
![License](https://img.shields.io/badge/License-MIT-green)

es una plataforma de an√°lisis de sentimientos desarrollada como prototipo funcional para la Maestr√≠a en Inteligencia de Negocios y Ciencia de Datos (UEES).

El proyecto implementa una **Arquitectura H√≠brida (Cross-Lingual)** innovadora: entrena un modelo Transformer de √∫ltima generaci√≥n (`xlm-roberta-base`) utilizando datasets masivos en **Ingl√©s** (Amazon Reviews), pero permite realizar inferencias y clasificaciones en **Espa√±ol** mediante una capa de traducci√≥n en tiempo real.

---

## üöÄ Caracter√≠sticas del Proyecto

* **Modelo SOTA:** Implementaci√≥n de `XLM-RoBERTa`, un modelo optimizado para tareas multiling√ºes.
* **Entrenamiento Robusto:** Fine-tuning realizado con +20,000 rese√±as reales de productos.
* **Inferencia H√≠brida:** Capacidad de recibir texto en espa√±ol, traducirlo internamente y clasificarlo con el motor anal√≠tico entrenado en ingl√©s.
* **Alta Precisi√≥n:** **Accuracy del 89.08%** validado en el conjunto de prueba.
* **Manejo de Incertidumbre:** L√≥gica de umbral para detectar rese√±as "Neutras" en un entorno de datos polarizados.

---

## üõ†Ô∏è Arquitectura T√©cnica

El flujo de datos dise√±ado para este prototipo maximiza el uso de recursos open-source disponibles:

```mermaid
graph LR
    A[Usuario (Input Espa√±ol)] -->|'El env√≠o demor√≥ mucho'| B(Capa de Traducci√≥n)
    B -->|'Shipping took too long'| C{Modelo XLM-RoBERTa}
    C -->|An√°lisis de Atenci√≥n| D[Clasificaci√≥n Softmax]
    D -->|Resultado Final| E[Negativo üò°]
Ingesta: El usuario ingresa una rese√±a en espa√±ol.

Pre-procesamiento: Normalizaci√≥n y traducci√≥n autom√°tica (ES -> EN) usando deep-translator.

Inferencia: El modelo predice la polaridad (Positivo, Neutro, Negativo).

Post-procesamiento: Aplicaci√≥n de reglas de negocio para refinar la clase neutra.

üìÇ Contenido del Repositorio
Este repositorio contiene los 3 Notebooks que componen el pipeline completo de ML:

üìò 01_EDA_Limpieza.ipynb:

Ingesta del dataset Amazon Product Reviews.

Limpieza de texto con Expresiones Regulares (Regex).

Estratificaci√≥n de datos (Train/Test Split).

üìô 02_Entrenamiento.ipynb:

Configuraci√≥n del Tokenizador AutoTokenizer.

Entrenamiento con la API Trainer de Hugging Face (GPU T4).

Persistencia del modelo entrenado.

üìó 03_Evaluacion_Inferencia.ipynb:

Evaluaci√≥n de m√©tricas (Matriz de Confusi√≥n, F1-Score).

Funci√≥n de predicci√≥n final para consumo del modelo con traducci√≥n integrada.

üíª Instalaci√≥n y Uso
Este proyecto est√° dise√±ado para ejecutarse en Google Colab. Si deseas correrlo localmente:

Clonar el repositorio:

Bash

git clone [https://github.com/tu-usuario/EcoSent-IA.git](https://github.com/tu-usuario/EcoSent-IA.git)
cd EcoSent-IA
Instalar dependencias:

Bash

pip install torch transformers accelerate datasets scikit-learn pandas deep-translator emoji
Ejecutar inferencia (Ejemplo en Python):

Python

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from deep_translator import GoogleTranslator
import torch

# Cargar modelo (aseg√∫rate de tener la carpeta del modelo descargada)
modelo_path = "./modelos/sentimiento_xlmroberta_v1"
tokenizer = AutoTokenizer.from_pretrained(modelo_path)
model = AutoModelForSequenceClassification.from_pretrained(modelo_path)

def analizar_sentimiento(texto):
    # Capa de traducci√≥n H√≠brida
    traductor = GoogleTranslator(source='es', target='en')
    texto_en = traductor.translate(texto)

    # Inferencia
    inputs = tokenizer(texto_en, return_tensors="pt", truncation=True, max_length=128)
    with torch.no_grad():
        logits = model(**inputs).logits
    return logits.argmax(-1).item()

print(analizar_sentimiento("¬°Me encant√≥ el producto, lleg√≥ rapid√≠simo!")) 
# Resultado esperado: 2 (Positivo)
